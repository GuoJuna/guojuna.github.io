(window.webpackJsonp=window.webpackJsonp||[]).push([[115],{524:function(a,t,s){"use strict";s.r(t);var r=s(0),e=Object(r.a)({},(function(){var a=this,t=a._self._c;return t("ContentSlotsDistributor",{attrs:{"slot-key":a.$parent.slotKey}},[t("h2",{attrs:{id:"简介"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#简介"}},[a._v("#")]),a._v(" 简介")]),a._v(" "),t("p",[a._v("Scrapy 是用 Python 实现的一个为了爬取网站数据、提取结构性数据而编写的应用框架。")]),a._v(" "),t("p",[a._v("Scrapy 常应用在包括数据挖掘，信息处理或存储历史数据等一系列的程序中。")]),a._v(" "),t("p",[a._v("通常我们可以很简单的通过 Scrapy 框架实现一个爬虫，抓取指定网站的内容或图片。")]),a._v(" "),t("h2",{attrs:{id:"架构图"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#架构图"}},[a._v("#")]),a._v(" 架构图")]),a._v(" "),t("p",[t("a",{attrs:{title:"",href:"https://gj-blog.oss-cn-hangzhou.aliyuncs.com/blog-img/2022-05-12-i3WRzo.png"}},[t("img",{attrs:{src:"https://gj-blog.oss-cn-hangzhou.aliyuncs.com/blog-img/2022-05-12-i3WRzo.png",alt:""}})])]),a._v(" "),t("ul",[t("li",[t("p",[a._v("Scrapy Engine(引擎): 负责Spider、ItemPipeline、Downloader、Scheduler中间的通讯，信号、数据传递等。")])]),a._v(" "),t("li",[t("p",[a._v("Scheduler(调度器): 它负责接受引擎发送过来的Request请求，并按照一定的方式进行整理排列，入队，当引擎需要时，交还给引擎。")])]),a._v(" "),t("li",[t("p",[a._v("Downloader（下载器）：负责下载Scrapy Engine(引擎)发送的所有Requests请求，并将其获取到的Responses交还给Scrapy Engine(引擎)，由引擎交给Spider来处理，")])]),a._v(" "),t("li",[t("p",[a._v("Spider（爬虫）：它负责处理所有Responses,从中分析提取数据，获取Item字段需要的数据，并将需要跟进的URL提交给引擎，再次进入Scheduler(调度器).")])]),a._v(" "),t("li",[t("p",[a._v("Item Pipeline(管道)：它负责处理Spider中获取到的Item，并进行进行后期处理（详细分析、过滤、存储等）的地方。")])])]),a._v(" "),t("h2",{attrs:{id:"工作原理"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#工作原理"}},[a._v("#")]),a._v(" 工作原理")]),a._v(" "),t("ol",[t("li",[a._v("引擎向spiders要url")]),a._v(" "),t("li",[a._v("引擎将要爬取的url给调度器")]),a._v(" "),t("li",[a._v("调度器会将url生成请求对象放入到指定的队列中")]),a._v(" "),t("li",[a._v("从队列中出队一个请求")]),a._v(" "),t("li",[a._v("引擎将请求交给下载器进行处理")]),a._v(" "),t("li",[a._v("下载器发送请求获取互联网数据")]),a._v(" "),t("li",[a._v("下载器将数据返回给引擎")]),a._v(" "),t("li",[a._v("引擎将数据再次给到spiders")]),a._v(" "),t("li",[a._v("spiders通过xpath解析该数据,得到数据或者url")]),a._v(" "),t("li",[a._v("spiders将数据或者rul给到引擎")]),a._v(" "),t("li",[a._v("引擎判断该数据还是url,数据交给管道处理,url交给调度器处理")])]),a._v(" "),t("h2",{attrs:{id:"基本使用"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#基本使用"}},[a._v("#")]),a._v(" 基本使用")]),a._v(" "),t("ol",[t("li",[a._v("创建爬虫项目")])]),a._v(" "),t("div",{staticClass:"language- extra-class"},[t("pre",{pre:!0,attrs:{class:"language-text"}},[t("code",[a._v("# scrapy startproject 项目名称 注意: 项目的名字不允许数字开头, 也不能包含中文\nscrapy startproject baidu\n")])])]),t("ol",{attrs:{start:"2"}},[t("li",[a._v("创建爬虫文件")])]),a._v(" "),t("div",{staticClass:"language- extra-class"},[t("pre",{pre:!0,attrs:{class:"language-text"}},[t("code",[a._v("# 进入爬虫文件夹\ncd baidu\\baidu\\spiders\n\n# scrapy genspider 爬虫名称 爬虫地址\nscrapy genspider baidu www.baidu.com\n\n# 创建链接跟进爬虫 \n# scrapy genspider -t crawl 爬虫文件的名字 爬虫的域名\n")])])]),t("ol",{attrs:{start:"3"}},[t("li",[a._v("运行爬虫代码")])]),a._v(" "),t("div",{staticClass:"language- extra-class"},[t("pre",{pre:!0,attrs:{class:"language-text"}},[t("code",[a._v("# scrapy crawl 爬虫的名字 \nscrapy crawl baidu\n")])])]),t("h2",{attrs:{id:"项目结构"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#项目结构"}},[a._v("#")]),a._v(" 项目结构")]),a._v(" "),t("p",[a._v("项目名字\n项目名字\nspiders文件夹 (存储的是爬虫文件)\ninit\n自定义的爬虫文件    核心功能文件\ninit\nitems           定义数据结构的地方 爬取的数据都包含哪些\nmiddleware      中间件     代理\npipelines       管道  用来处理下载的数据\nsettings        配置文件    robots协议    ua定义等")]),a._v(" "),t("h2",{attrs:{id:"代码实例"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#代码实例"}},[a._v("#")]),a._v(" 代码实例")]),a._v(" "),t("p",[t("a",{attrs:{href:"https://github.com/GuoJuna/scrapy_study/tree/master/scrapy_039_readbook",target:"_blank",rel:"noopener noreferrer"}},[a._v("读书网爬取"),t("OutboundLink")],1)])])}),[],!1,null,null,null);t.default=e.exports}}]);